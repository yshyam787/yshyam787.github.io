<div class="project-crc">
    <h1>Collaborative Research Center (CRC) Development</h1>
    <img id="sfb-1528" src="assets/media/sfb_1528.jpg" alt="SFB-1528 Project">
    <p>I am currently working on the development of the CRC with integrated research data management and processing solutions for the SFB-1528 project 
        <a href="https://www.uni-goettingen.de/de/vorschau_4d7abdea6865ad9b00e5ffc2312b6c31/675774.html">(Read more)</a>. 
        Essentially, German researchers located in different regions of the world are conducting research on "Cognition of Interaction." 
        They observe the behavior of primates and collect data in CSV and Excel files, such as <i>Demographic_Data.csv</i> and <i>Hormone_Information.csv</i>. 
        However, these datasets contain numerous errors and abnormalities. 
        To address this, I am developing a centralized solution for this project by creating a concrete data pipeline. 
        Researchers can simply dump their data on OwnCloud and perform analysis on Metabase, while all intermediate steps are handled and automated by me. 
        The processes and data science solutions involved in this project are explained below.
    </p>

    <ol>
        <li>Rclone is implemented for the automated ingestion of data from OwnCloud via WebDAV. Data dumped on OwnCloud is automatically synced to the local machine/directory for further processing.</li>

        <li>Databases and tables in PostgreSQL are created based on the names of folders, subfolders, and files.</li>

        <li>In the <span style="background-color: lightgrey;">metadata.json</span> file, the schema for the database and data types is defined. Pandas is used to create a dataframe based on the JSON file.</li>

        <li>After preprocessing the dataframe, it is pushed to the database. <span style="background-color: lightgrey;">Alembic</span> is used for versioning the schema in PostgreSQL.</li>

        <li><span style="background-color: lightgrey;">Cronjob</span> and <span style="background-color: lightgrey;">Celery</span> are used for the automation of the pipeline.</li>
    </ol>
</div>


<div class="project-xwiki">
    <h1>XWiki Development and Integration with WordPress</h1>
    <img id="xwiki-img" src="assets/media/xwiki.png" alt="XWiki Interface">
    <p>Considering its flexibility, robust built-in Solr search capability, and user-friendly UI, 
        I embedded <a href="https://c110-019.cloud.gwdg.de/bin/view/G%C3%B6ttinger%20Digitale%20Akademie/">XWiki</a> 
        as a centralized solution for the Knowledge Base 
        <a href="https://c110-019.cloud.gwdg.de/bin/view/G%C3%B6ttinger%20Digitale%20Akademie/">(Read more)</a>. 
        All information regarding the projects of the Nieders√§chsische Digitale Akademie and related details 
        was stored in this XWiki. The entire development of this project is explained below.
    </p>

    <ol>
        <li>XWiki and its Standard Flavor were installed through Docker Compose on the Ubuntu Virtual Machine of the Academic Cloud at GWDG.</li>

        <li>XWiki was deployed on Apache Tomcat, and SSL certificates were generated through Let's Encrypt using Caddy.</li>

        <li>After creating the pages and sub-pages, the spaces were integrated into WordPress using the XWiki REST API.</li>

        <li>Customized plugins for WordPress were created using PHP. These plugins fetched the contents of XWiki Spaces and integrated them into WordPress.</li>

        <li>Pandoc was used for data migration from the WordPress Encyclopedia to XWiki. A complete data migration and integration pipeline was built.</li>
    </ol>
</div>


<div class="project-hekksagon">
    <h1>HeKKSaGOn-GPDS Summer School Projects on Data Science</h1>
    <img id="hekksagon-img" src="assets/media/HeKKSaGOnCertificate.jpeg" alt="HeKKSaGOn-GPDS Certificate">
    <p>In the summer of 2024, I was selected for the 
        <a href="https://sites.google.com/view/datasciencesummerschool2024?usp=sharing">International Summer School on Data Science 2024</a> 
        program organized by the <a href="https://www.hekksagon.net/232.php">HeKKSaGOn</a> network at <a href="https://gp-ds.tohoku.ac.jp/en/events/seminar/tohoku-university-data-science-summer-school-2024.html">Tohoku University</a>, Sendai, Japan</a>. 
        It was a two-week intensive summer school program covering a wide range of topics in advanced data science. 
        Additionally, I worked on numerous workshops and successfully won three Kaggle competitions. 
        The workshops I participated in are listed below.
    </p>

    <ol>
        <li>I participated in the 'Lemonade Stand Profit Prediction' Kaggle competition. 
            Based on the ten given parameters, my team and I predicted the profit. 
            Various dimensionality reduction techniques and regression algorithms were used. 
            <a href="https://github.com/yshyam787/HeKKSaGOn-GPDS_Projects/blob/main/Kaggle_Competitions/Lemonade_Stand_income_prediction_using_data_simplification.ipynb">Read more</a>
        </li>

        <li>Similarly, the next workshop was on the 'Stock Price Forecasting' competition, where we needed to forecast the stock price for the second half of 2020. 
            Time series analysis methods, including ARIMA, SARIMA, and LSTM models, were implemented to predict future trends. 
            <a href="https://github.com/yshyam787/HeKKSaGOn-GPDS_Projects/blob/main/Kaggle_Competitions/Stock_price_prediction_of_second_half_of_2020%20_Time_Series_Analysis.ipynb">Read more</a>
        </li>

        <li>The third workshop was on the 'Golfer Type Clustering' competition, where clustering algorithms, including K-means and DBSCAN, were implemented. 
            <a href="https://github.com/yshyam787/HeKKSaGOn-GPDS_Projects/blob/main/Kaggle_Competitions/Golfer_type_clustering.ipynb">Read more</a>
        </li>
    </ol>
</div>


<div class="project-modeling">
    <h1>World Trade Database Deduping</h1>

    <img id="pandas-img" src="assets/media/pandas.png" alt="Pandas Dedupe">
    <p>During my internship at <a href="https://makersite.io/">Makersite GmbH</a>, I worked on the World Trade Database project, 
        where database deduping was implemented using the <code>pandas_dedupe</code> machine learning model. 
        <a href="https://github.com/yshyam787/World_Trade_Database_Deduping/blob/master/Code/main_script.py">Read more...</a>
    </p>

    <ol>
        <li>The database library was extremely large, containing multiple folders and subfolders. 
            All these directories were traversed using the <code>os</code> library, and the paths of the files and folders were navigated and stored.
        </li>

        <li>Before modeling the data, it was preprocessed and cleaned using <code>pandas_dedupe</code>. 
            The model was first trained and then applied throughout the database. 
            It took more than 10 hours to dedupe the entire database.
        </li>

        <li>Before finalizing the perfect model, the data was also trained on various other libraries 
            using machine learning tools such as TensorFlow and PyTorch.
        </li>
    </ol>
</div>


<div class="project-credit">
    <h1>Credit Card Fraud Detection</h1>

    <img id="fraud-detection" src="assets/media/fraud_detection.png" alt="Credit Card Fraud Detection">

    <p>This is my personal project. I was very interested in learning the mechanism of fraud detection and clustering algorithms. 
        Therefore, I worked on credit transactions made by European cardholders in September 2013. 
        The dataset covered a period of 48 hours with a total of 284,807 transactions. 
        <a href="https://github.com/yshyam787/Credit_Card_Fraud_Detection/blob/main/credit_card_fraud_detection.ipynb">Read more...</a>
    </p>

    <ol>
        <li>The data was cleaned and preprocessed using <code>Pandas</code> and <code>NumPy</code>. 
            Preprocessing involved dropping null values and duplicates, transformation, sorting, and feature extraction.
        </li>

        <li>Preprocessed data was visualized using Matplotlib, and the general trend of the highest and lowest transaction times of the day was analyzed.
        </li>

        <li>Monitoring rules, such as the threshold amount rule and threshold frequency within an hour, were specified. 
            Fraudulent customers were flagged and detected based on a combined analysis of transaction amounts and transaction frequencies within an hour.
        </li>
    </ol>
</div>
